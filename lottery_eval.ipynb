{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Utils import load\n",
    "from Utils import generator\n",
    "from Utils import metrics\n",
    "from train import *\n",
    "from prune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cifar10 dataset.\n",
      "Creating lottery-vgg16 model.\n"
     ]
    }
   ],
   "source": [
    "## Random Seed and Device ##\n",
    "torch.manual_seed(1)\n",
    "device = load.device(0)\n",
    "\n",
    " ## Model, Loss, Optimizer ##\n",
    "print('Loading {} dataset.'.format('cifar10'))\n",
    "input_shape, num_classes = load.dimension('cifar10') \n",
    "    \n",
    "print('Creating {}-{} model.'.format('lottery', 'vgg16'))\n",
    "model = load.model('vgg16', 'lottery')(input_shape, \n",
    "                                       num_classes, \n",
    "                                    False, \n",
    "                                    pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.conv.weight\n",
      "layers.0.conv.bias\n",
      "layers.1.conv.weight\n",
      "layers.1.conv.bias\n",
      "layers.3.conv.weight\n",
      "layers.3.conv.bias\n",
      "layers.4.conv.weight\n",
      "layers.4.conv.bias\n",
      "layers.6.conv.weight\n",
      "layers.6.conv.bias\n",
      "layers.7.conv.weight\n",
      "layers.7.conv.bias\n",
      "layers.8.conv.weight\n",
      "layers.8.conv.bias\n",
      "layers.10.conv.weight\n",
      "layers.10.conv.bias\n",
      "layers.11.conv.weight\n",
      "layers.11.conv.bias\n",
      "layers.12.conv.weight\n",
      "layers.12.conv.bias\n",
      "layers.14.conv.weight\n",
      "layers.14.conv.bias\n",
      "layers.15.conv.weight\n",
      "layers.15.conv.bias\n",
      "layers.16.conv.weight\n",
      "layers.16.conv.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "0.05 compression ratio, 2 train-prune levels\n",
      "Top1 Accuracy:  88.56\n",
      "Time:  0.7426261589862406\n",
      "Parameter Sparsity: 13119513/14719818 (0.8913)\n",
      "FLOP Sparsity: 291206301/313478154 (0.9290)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.05 compression ratio, 3 train-prune levels\n",
      "Top1 Accuracy:  88.65\n",
      "Time:  0.7002086709835567\n",
      "Parameter Sparsity: 13119513/14719818 (0.8913)\n",
      "FLOP Sparsity: 290160432/313478154 (0.9256)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.05 compression ratio, 4 train-prune levels\n",
      "Top1 Accuracy:  89.81\n",
      "Time:  0.688524596975185\n",
      "Parameter Sparsity: 13119513/14719818 (0.8913)\n",
      "FLOP Sparsity: 289306655/313478154 (0.9229)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.1 compression ratio, 2 train-prune levels\n",
      "Top1 Accuracy:  89.15\n",
      "Time:  0.6461811800254509\n",
      "Parameter Sparsity: 11693236/14719818 (0.7944)\n",
      "FLOP Sparsity: 271328043/313478154 (0.8655)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.1 compression ratio, 3 train-prune levels\n",
      "Top1 Accuracy:  88.65\n",
      "Time:  0.6337619450059719\n",
      "Parameter Sparsity: 11693237/14719818 (0.7944)\n",
      "FLOP Sparsity: 269978550/313478154 (0.8612)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.1 compression ratio, 4 train-prune levels\n",
      "Top1 Accuracy:  89.06\n",
      "Time:  0.6526846549822949\n",
      "Parameter Sparsity: 11693237/14719818 (0.7944)\n",
      "FLOP Sparsity: 268348804/313478154 (0.8560)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.2 compression ratio, 2 train-prune levels\n",
      "Top1 Accuracy:  88.84\n",
      "Time:  0.6379275030340068\n",
      "Parameter Sparsity: 9289140/14719818 (0.6311)\n",
      "FLOP Sparsity: 237124342/313478154 (0.7564)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.2 compression ratio, 3 train-prune levels\n",
      "Top1 Accuracy:  89.2\n",
      "Time:  0.6549057950032875\n",
      "Parameter Sparsity: 9289140/14719818 (0.6311)\n",
      "FLOP Sparsity: 235275627/313478154 (0.7505)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.2 compression ratio, 4 train-prune levels\n",
      "Top1 Accuracy:  89.69\n",
      "Time:  0.6656641209847294\n",
      "Parameter Sparsity: 9289138/14719818 (0.6311)\n",
      "FLOP Sparsity: 232417138/313478154 (0.7414)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.5 compression ratio, 2 train-prune levels\n",
      "Top1 Accuracy:  89.14\n",
      "Time:  0.7395466209854931\n",
      "Parameter Sparsity: 4657711/14719818 (0.3164)\n",
      "FLOP Sparsity: 162651231/313478154 (0.5189)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.5 compression ratio, 3 train-prune levels\n",
      "Top1 Accuracy:  89.37\n",
      "Time:  0.6715831360197626\n",
      "Parameter Sparsity: 4657711/14719818 (0.3164)\n",
      "FLOP Sparsity: 160648071/313478154 (0.5125)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "0.5 compression ratio, 4 train-prune levels\n",
      "Top1 Accuracy:  88.94\n",
      "Time:  0.6412896740366705\n",
      "Parameter Sparsity: 4657711/14719818 (0.3164)\n",
      "FLOP Sparsity: 160342732/313478154 (0.5115)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "1.0 compression ratio, 2 train-prune levels\n",
      "Top1 Accuracy:  89.52\n",
      "Time:  0.6490121480310336\n",
      "Parameter Sparsity: 1475792/14719818 (0.1003)\n",
      "FLOP Sparsity: 66883739/313478154 (0.2134)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "1.0 compression ratio, 3 train-prune levels\n",
      "Top1 Accuracy:  89.57\n",
      "Time:  0.6983353029936552\n",
      "Parameter Sparsity: 1475792/14719818 (0.1003)\n",
      "FLOP Sparsity: 68744961/313478154 (0.2193)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "1.0 compression ratio, 4 train-prune levels\n",
      "Top1 Accuracy:  89.8\n",
      "Time:  0.6415760489762761\n",
      "Parameter Sparsity: 1475792/14719818 (0.1003)\n",
      "FLOP Sparsity: 71475689/313478154 (0.2280)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "2.0 compression ratio, 2 train-prune levels\n",
      "Top1 Accuracy:  81.6\n",
      "Time:  0.6377505499986\n",
      "Parameter Sparsity: 151390/14719818 (0.0103)\n",
      "FLOP Sparsity: 8971465/313478154 (0.0286)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "2.0 compression ratio, 3 train-prune levels\n",
      "Top1 Accuracy:  75.37\n",
      "Time:  0.6525724290404469\n",
      "Parameter Sparsity: 151389/14719818 (0.0103)\n",
      "FLOP Sparsity: 7685130/313478154 (0.0245)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################\n",
      "2.0 compression ratio, 4 train-prune levels\n",
      "Top1 Accuracy:  19.34\n",
      "Time:  0.6670184649992734\n",
      "Parameter Sparsity: 151389/14719818 (0.0103)\n",
      "FLOP Sparsity: 7493548/313478154 (0.0239)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "result_dir = 'Results/data/multishot/1'\n",
    "\n",
    "for compression in [0.05, 0.1, 0.2, 0.5, 1.0, 2.0]:\n",
    "\tfor level in [2, 3, 4]:\n",
    "\t\tprint((\"##############################################\"))\n",
    "\t\tprint(f'{compression} compression ratio, {level} train-prune levels')\n",
    "\t\t\n",
    "\t\tcompress_file = f\"{result_dir}/compression-mag-{compression}-{level}.pkl\"\n",
    "\t\tprune_result = pd.read_pickle(compress_file)\n",
    "\n",
    "\t\tpost_file = f\"{result_dir}/post-train-mag-{compression}-{level}.pkl\"\n",
    "\t\tpost_results = pd.read_pickle(post_file)\n",
    "\n",
    "\t\tprint(\"Top1 Accuracy: \", post_results.iloc[-1]['top1_accuracy'])\n",
    "\t\tprint(\"Time: \", post_results.iloc[-1]['time'])\n",
    "\t\t# print(\"FLOP: \", prune_result['flops'])\n",
    "\n",
    "\t\ttotal_params = int((prune_result['sparsity'] * prune_result['size']).sum())\n",
    "\t\tpossible_params = prune_result['size'].sum()\n",
    "\t\ttotal_flops = int((prune_result['sparsity'] * prune_result['flops']).sum())\n",
    "\t\tpossible_flops = prune_result['flops'].sum()\n",
    "\t\t# print(\"Train results:\\n\", train_result)\n",
    "\t\t# print(\"Prune results:\\n\", prune_result)\n",
    "\t\tprint(\"Parameter Sparsity: {}/{} ({:.4f})\".format(total_params, possible_params, total_params / possible_params))\n",
    "\t\tprint(\"FLOP Sparsity: {}/{} ({:.4f})\".format(total_flops, possible_flops, total_flops / possible_flops))\n",
    "\t\tprint('\\n\\n\\n\\n') \n",
    "                                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
